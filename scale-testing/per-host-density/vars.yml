# Per-Host Density Test Configuration Variables
# This file defines all variables used in the per-host-density test
#
# Usage Examples:
#
# Simplest - uses run-workloads.sh wrapper (recommended):
#   ./run-workloads.sh
#
# Single-node mode (all VMs on one node):
#   targetNode=worker001 vmsPerNamespace=400 ./run-workloads.sh
#
# Multi-node mode (distribute VMs across all workers):
#   scaleMode=multi-node vmsPerNamespace=400 ./run-workloads.sh
#
# Multiple namespaces, single node (800 VMs: 2 ns × 400 VMs):
#   targetNode=worker001 namespaceCount=2 vmsPerNamespace=400 ./run-workloads.sh
#
# Multiple namespaces, multi-node (1200 VMs: 3 ns × 400 VMs, distributed):
#   scaleMode=multi-node namespaceCount=3 vmsPerNamespace=400 ./run-workloads.sh
#
# NOTE: Variable names are case-sensitive!
# Common mistakes to avoid:
#   vmspernamespace=100 ./run-workloads.sh       # WRONG: lowercase won't work
#   SCALEMODE=multi-node ./run-workloads.sh      # WRONG: uppercase won't work
#
# Advanced - direct kube-burner (requires manual timestamp):
#   runTimestamp=$(date +"run-%Y%m%d-%H%M%S") vmsPerNamespace=100 kube-burner init --config=per-host-density.yml --user-data=vars.yml
#

# Test Configuration
testName: 'per-host-density'            # Test identifier used for labeling and metrics
resultsPath: "/tmp/kube-burner-results/per-host-density"  # Base directory for test results
runTimestamp: "run-default"             # Auto-generated by run-workloads.sh

# Scale Mode Configuration
scaleMode: "single-node"                # Options: "single-node" or "multi-node"
                                        # single-node: All VMs pinned to targetNode (current behavior)
                                        # multi-node: VMs distributed across worker nodes

# Namespace Configuration
namespaceCount: 15                       # Number of namespaces to create (uses jobIterations)
testNamespacePrefix: "per-host-density-test"  # Namespace naming prefix (ns-0, ns-1, etc. appended)

# Validation Configuration
percentage_of_vms_to_validate: 50       # Percentage of VMs to randomly select for SSH validation (default 25%)
                                        # Example: vmsPerNamespace=400, percentage=25 → 100 VMs validated via SSH
                                        # Set to 0 to disable SSH validation entirely
max_ssh_retries: 240                      # Maximum retries for SSH connection per VM (retry every 15s, default 8 = 2 minutes max wait)

# VM Scaling Configuration  
vmsPerNamespace: 30                   # Number of VMs to create per namespace
# Note: vmsPerHost is deprecated - use vmsPerNamespace instead
vmUser: alpine                          # SSH username for Alpine Linux VMs
# Storage Configuration
sourceStorageSize: 5120                  # Source DataVolume size in MiB for VM cloning
vmStorageSize: 5120                      # Individual VM disk size in MiB
storageClassName: ocs-storagecluster-ceph-rbd  # Storage class for PVC provisioning

# VM Resource Allocation
vmCpuCores: 100                        # VM CPU cores (deprecated, use vmCpuRequest/vmCpuLimit)
vmMemory: 256Mi                         # Memory allocation per VM (optimized for Alpine Linux density)
vmCpuRequest: "100m"                   # Minimum CPU guarantee per VM (100 milliCPU)
vmCpuLimit: "1000m"                     # Maximum CPU limit per VM (800 milliCPU)

# Test Execution Control
counter: 1                             # Test iteration counter (0 triggers cleanup)
max-iterations: 1                      # Maximum number of test iterations
cleanup: true                          # Whether to clean up resources after test

# VM Lifecycle Skip Options
skipVmShutdown: false                  # Set to true to skip VM shutdown phase
skipVmRestart: false                   # Set to true to skip VM restart/startup phase

# SSH Configuration for VM Access
privateKey: '/home/kni/.ssh/id_rsa'          # Private key path for VM SSH access
publicKey: '/home/kni/.ssh/id_rsa.pub'       # Path to SSH public key

# Node Targeting Configuration
# Single-node mode settings (used when scaleMode: "single-node")
targetNode: ""                          # Specific worker hostname for VM pinning (e.g., "worker001")
nodeSelectorLabel: "kubernetes.io/hostname"     # Label used for node selection

# Multi-node mode settings (used when scaleMode: "multi-node")
workerNodeSelector: "node-role.kubernetes.io/worker="  # Label selector for target nodes

# VM Lifecycle Configuration
shutdownBatchSize: 50                  # Number of VMs to shutdown simultaneously
sleepBetweenPhases: 2m                 # Wait time between shutdown and startup phases

# Rate Limiting - Creation Phase
qpsCreate: 20                          # Queries per second for VM creation
burstCreate: 40                        # Burst limit for VM creation

# Rate Limiting - Shutdown Phase  
qpsShutdown: 10                        # Queries per second for VM shutdown
burstShutdown: 20                      # Burst limit for VM shutdown

# Rate Limiting - Startup Phase
qpsStartup: 30                         # Queries per second for VM startup  
burstStartup: 60                       # Burst limit for VM startup

# Timeout Configuration
maxWaitTimeout: 2h                     # Maximum wait time for resource readiness
jobPause: 1m                           # Pause between job phases

# Monitoring Configuration (Optional)
esServer: http://f01-h08-000-1029u.rdu2.scalelab.redhat.com:9200  # Elasticsearch server for metrics storage
PROM: ""                               # Prometheus endpoint (set via environment to enable monitoring)
PROM_TOKEN: ""                         # Prometheus token (set via environment to enable monitoring)

# VM Image Configuration
imageUrl: "https://dl-cdn.alpinelinux.org/alpine/v3.22/releases/cloud/generic_alpine-3.22.0-x86_64-bios-cloudinit-r0.qcow2"  # Alpine Linux cloud image URL for VM creation